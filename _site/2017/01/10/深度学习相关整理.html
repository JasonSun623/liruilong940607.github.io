<!DOCTYPE html> <html> <head> <meta charset="utf-8"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" /> <!--[if lte IE 9]><meta http-equiv="refresh" content="0;url=/ie.html"><![endif]--> <meta name="author" content="Ruilong Li"> <meta name="keywords" content="segmentation"> <link href='http://fonts.googleapis.com/css?family=Open+Sans:400,400italic,600|Droid+Sans+Mono' rel='stylesheet' type='text/css'> <link rel="shortcut icon" href="" /> <link rel="stylesheet" href="/css/main.css"> <link rel="canonical" href="http://localhost:4000/2017/01/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%95%B4%E7%90%86.html"> <!-- Begin Jekyll SEO tag v2.5.0 --> <title>深度学习相关整理 | Dalong’s Blog</title> <meta name="generator" content="Jekyll v3.8.3" /> <meta property="og:title" content="深度学习相关整理" /> <meta name="author" content="Ruilong Li" /> <meta property="og:locale" content="en_US" /> <meta name="description" content="1. 背景 三维场景kinect采集，点云重构及分割" /> <meta property="og:description" content="1. 背景 三维场景kinect采集，点云重构及分割" /> <link rel="canonical" href="http://localhost:4000/2017/01/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%95%B4%E7%90%86.html" /> <meta property="og:url" content="http://localhost:4000/2017/01/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%95%B4%E7%90%86.html" /> <meta property="og:site_name" content="Dalong’s Blog" /> <meta property="og:type" content="article" /> <meta property="article:published_time" content="2017-01-10T00:00:00+08:00" /> <script type="application/ld+json"> {"description":"1. 背景 三维场景kinect采集，点云重构及分割","author":{"@type":"Person","name":"Ruilong Li"},"@type":"BlogPosting","url":"http://localhost:4000/2017/01/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%95%B4%E7%90%86.html","headline":"深度学习相关整理","dateModified":"2017-01-10T00:00:00+08:00","datePublished":"2017-01-10T00:00:00+08:00","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/2017/01/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%95%B4%E7%90%86.html"},"@context":"http://schema.org"}</script> <!-- End Jekyll SEO tag --> <link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="Dalong's Blog" /> <script src="/assets/js/prefixfree.js"></script> </head> <body> <aside id="sidebar" class="open"> <div id="sidebar-left"> <a id="sidebar-avatar" href="/"> <img id="sidebar-avatar-img" alt="" src="http://static.zybuluo.com/lrl940607/pa3uubxkdsai8ct7htwq7giu/photo.png"/> </a> <div id="sidebar-social"> <a href="/pages/feed.xml" class="sidebar-social-icon feed"></a> <a href="mailto:li-rl16@mails.tsinghua.edu.cn" class="sidebar-social-icon email"></a> <!-- Generate icon by yourself https://icomoon.io/app/#/select --> <a href="https://github.com/liruilong940607" class="sidebar-social-icon github" target="_blank"></a> </div> <ul id="sidebar-tags"> <li class="sidebar-tag active" data-filter="recent">newest</li> <li class="sidebar-tag" data-filter="course">course</li> <li class="sidebar-tag" data-filter="tutorial">tutorial</li> <li class="sidebar-tag" data-filter="lab">lab</li> </ul> </div> <div id="sidebar-right"> <div id="search-box"> <input id="search-input" type="text" placeholder="Search" /> </div> <nav id="toc"> <a class="toc-link" data-tags="tutorial" href="/2018/01/05/python-jupyter-%E5%AE%89%E8%A3%85%E7%AE%80%E6%98%8E%E6%95%99%E7%A8%8B.html"> windows python & jupyter 安装简明教程 </a> <a class="toc-link" data-tags="tutorial" href="/2017/09/01/PyQt5%E7%9A%84%E5%AE%89%E8%A3%85-Mac%E7%89%88.html"> PyQt5的安装（Mac版） </a> <a class="toc-link" data-tags="tutorial" href="/2017/09/01/Mac%E4%B8%8Bopencv2+python2.7-&-opencv3+python3.6%E7%9A%84%E9%85%8D%E7%BD%AE.html"> Mac下opencv2+python2.7 & opencv3+python3.6的配置 </a> <a class="toc-link" data-tags="tutorial" href="/2017/08/31/Mac-%E6%96%B0%E7%B3%BB%E7%BB%9F%E7%9A%84%E9%85%8D%E7%BD%AE.html"> Mac 新系统的配置 </a> <a class="toc-link" data-tags="tutorial" href="/2017/08/25/%E4%BB%8E%E4%B9%B0%E6%9C%8D%E5%8A%A1%E5%99%A8%E5%88%B0%E8%87%AA%E6%90%ADshadowsocks%E7%A7%91%E5%AD%A6%E4%B8%8A%E7%BD%91.html"> 从买服务器到自搭shadowsocks科学上网  </a> <a class="toc-link" data-tags="lab" href="/2017/07/15/%E4%BA%BA%E5%83%8F%E6%8A%A0%E5%9B%BE%E4%BB%BB%E5%8A%A1%E6%89%8B%E5%86%8Cv2.html"> 人像抠图任务手册v2 </a> <a class="toc-link" data-tags="lab" href="/2017/06/19/Flickr%E4%BA%BA%E5%83%8F%E6%8A%A0%E5%9B%BE%E4%BB%BB%E5%8A%A1-%E7%AD%9B%E5%9B%BE%E6%89%8B%E5%86%8C.html"> Flickr人像抠图任务【筛图手册】 </a> <a class="toc-link" data-tags="course" href="/2017/06/05/%E4%B8%89%E7%BB%B4%E7%82%B9%E9%9B%86%E4%B8%AD%E6%9C%80%E5%A4%A7%E7%A9%BA%E5%BF%83%E7%90%83%E7%A8%8B%E5%BA%8F%E8%AF%B4%E6%98%8E%E6%96%87%E6%A1%A3.html"> 三维点集中最大空心球程序说明文档 </a> <a class="toc-link" data-tags="lab" href="/2017/04/25/CCF-%E8%A7%86%E8%A7%89-%E5%9B%BE%E5%BD%A2%E5%AD%A6-%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD-%E9%A1%B6%E7%BA%A7%E4%BC%9A%E8%AE%AE.html"> CCF 视觉、图形学、人工智能 顶级会议 </a> <a class="toc-link" data-tags="course" href="/2017/04/16/%E9%80%89%E9%A2%98%E6%8A%A5%E5%91%8A-%E5%BE%AE%E4%BF%A1%E8%81%8A%E5%A4%A9%E8%AE%B0%E5%BD%95%E6%95%B0%E6%8D%AE%E5%8F%AF%E8%A7%86%E5%8C%96.html"> 选题报告--微信聊天记录数据可视化 </a> <a class="toc-link" data-tags="lab" href="/2017/02/18/%E5%A6%82%E4%BD%95%E5%BF%AB%E9%80%9F%E5%9C%B0%E5%81%9A%E5%9B%BE%E7%89%87%E7%9A%84%E5%89%8D%E8%83%8C%E6%99%AF%E6%A0%87%E6%B3%A8.html"> 如何快速地做图片的前背景标注？ </a> <a class="toc-link" data-tags="course" href="/2017/01/15/%E4%B8%93%E9%A2%98%E5%9B%9B-CDN%E6%95%B0%E6%8D%AE%E5%8F%AF%E8%A7%86%E5%8C%96%E4%B8%8E%E6%95%85%E9%9A%9C%E6%A3%80%E6%B5%8B.html"> 专题四：CDN数据可视化与故障检测 </a> <a class="toc-link" data-tags="lab" href="/2017/01/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%95%B4%E7%90%86.html"> 深度学习相关整理 </a> <a class="toc-link" data-tags="lab" href="/2017/01/09/cs231n-%E6%96%AF%E5%9D%A6%E7%A6%8F%E8%AF%BE%E7%A8%8B.html"> cs231n 斯坦福课程 </a> <a class="toc-link" data-tags="course" href="/2016/12/29/%E7%A5%9E%E7%BB%8F%E4%B8%8E%E8%AE%A4%E7%9F%A5-Image-Segmentation-Using-a-Sparse-Coding-Model-of-Cortical-Area-V1.html"> 神经与认知：Image Segmentation Using a Sparse Coding Model of Cortical Area V1 </a> <a class="toc-link" data-tags="course" href="/2016/12/01/SNNs(Spiking-Neural-Networks)%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0.html"> SNNs(Spiking Neural Networks)学习笔记 </a> <a class="toc-link" data-tags="lab" href="/2016/11/27/RGB-D-dataset.html"> RGB-D dataset </a> <a class="toc-link" data-tags="lab" href="/2016/11/24/SUN-RGB-D-dataset.html"> SUN RGB-D dataset </a> <a class="toc-link" data-tags="lab" href="/2016/11/24/B3DO-Berkeley-3-D-Object-Dataset.html"> (B3DO) Berkeley 3-D Object Dataset </a> <a class="toc-link" data-tags="lab" href="/2016/11/23/Paper%E7%AC%94%E8%AE%B0-Segmentation%E7%9B%B8%E5%85%B3%E7%9A%84%E4%B8%A4%E7%AF%87%E6%96%87%E7%AB%A0.html"> Paper笔记：Segmentation相关的两篇文章 </a> <a class="toc-link" data-tags="lab" href="/2016/11/22/%E8%AE%B0%E5%BD%95-%E4%BD%BF%E7%94%A8py-faster-rcnn%E5%9C%A8%E8%87%AA%E5%B7%B1%E7%9A%84%E6%95%B0%E6%8D%AE%E9%9B%86%E4%B8%8A.html"> 记录：使用py-faster-rcnn在自己的数据集上 </a> <a class="toc-link" data-tags="lab" href="/2016/11/20/Paper%E7%AC%94%E8%AE%B0-SemanticFusion.html"> Paper笔记：SemanticFusion </a> <a class="toc-link" data-tags="lab" href="/2016/11/20/Paper%E7%AC%94%E8%AE%B0-Deconvolution-Network.html"> Paper笔记：Deconvolution Network </a> <a class="toc-link" data-tags="tutorial" href="/2016/11/18/caffe-configuration-on-ubuntu-14.04.html"> caffe ubuntu 14.04 -- 2016.11.18 </a> <a class="toc-link" data-tags="course" href="/2016/11/10/%E6%B5%81%E5%AA%92%E4%BD%93%E6%8A%80%E6%9C%AF-%E4%B8%93%E9%A2%98%E5%9B%9B.html"> 流媒体技术--专题四 </a> </nav> </div> </aside> <main id="main" class="open"> <article class="post container"> <div class="post-meta"> <span class="post-meta-span date">2017 January 10</span> </div> <h1 class="post-title">深度学习相关整理</h1> <h2 id="1-背景">1. 背景</h2> <ul> <li>三维场景kinect采集，点云重构及分割</li> </ul> <h2 id="2-数据集">2. 数据集</h2> <h3 id="b3do-berkeley-3-d-object-dataset">B3DO: Berkeley 3-D Object Dataset</h3> <ul> <li><a href="http://kinectdata.com/">[Project Page]</a></li> <li>A. Janoch, S. Karayev, Y. Jia, J.T. Barron, M. Fritz, K. Saenko, T. Darrell. A Category-Level 3-D Object Dataset: Putting the Kinect to Work. ICCV Workshop on Consumer Depth Cameras in Computer Vision 2011.</li> <li>single task : bbox detection ![detection.png-142.3kB][1.1]</li> <li>data distrubution ![data_distrubution.png-55.4kB][1.2]</li> <li>what data looks like; ![data_looks_like.png-216.3kB][1.3] [1.1]: http://static.zybuluo.com/lrl940607/2cb0j16y03ppa3flr8jafd34/detection.png [1.2]: http://static.zybuluo.com/lrl940607/xz64p9x5vproqfwot6qbxu7n/data_distrubution.png [1.3]: http://static.zybuluo.com/lrl940607/69hyispei8gyfdbfq320dgrs/data_looks_like.png</li> </ul> <h3 id="sun-rgb-d-dataset">SUN RGB-D dataset</h3> <ul> <li><a href="http://rgbd.cs.princeton.edu/">[Project Page]</a></li> <li>S. Song, S. Lichtenberg, and J. Xiao. SUN RGB-D: A RGB-D Scene Understanding Benchmark Suite Proceedings of 28th IEEE Conference on Computer Vision and Pattern Recognition (CVPR2015)</li> <li>SUNRGBD V1 : This file contains the 10335 RGBD images of SUNRGBD V1.</li> <li>datasize ![datasize.png-19kB][2.1]</li> <li>comparison ![comparison.png-43.2kB][2.2]</li> <li>seneor type: 4 ![sensors.png-67kB][2.3]</li> <li>multi-task ![mult-task.png-61.9kB][2.4]</li> <li>Object Category distribution ![distribution.png-62.6kB][2.5] -Scene Category distribution ![scene-distrubution.png-123.5kB][2.6] [2.1]: http://static.zybuluo.com/lrl940607/3d7jnx4ebshs19ljsfot2ry2/datasize.png [2.2]: http://static.zybuluo.com/lrl940607/jcnbes4bibx90eteqb86y4ui/comparison.png [2.3]: http://static.zybuluo.com/lrl940607/tgyce82betv8liqd4deyfisz/sensors.png [2.4]: http://static.zybuluo.com/lrl940607/0fuznddh9gjs2czb1op9t6me/mult-task.png [2.5]: http://static.zybuluo.com/lrl940607/os47ko3eiu83fo55thjfcsza/distribution.png [2.6]: http://static.zybuluo.com/lrl940607/ht65pep4vedliml8hlao6ql7/scene-distrubution.png</li> </ul> <h2 id="3论文工作">3.论文工作</h2> <h3 id="fully-convolutional-networks-for-semantic-segmentationfcn-cvpr-2015github链接"><strong>Fully Convolutional Networks for Semantic Segmentation(FCN) @CVPR 2015</strong><a href="https://github.com/shelhamer/fcn.berkeleyvision.org">[Github链接]</a></h3> <p>先上图看效果，以下均是用@杨晟的实拍数据，paper中没有自带RGB-D的demo测试图片。 ![0.png-388.9kB][3.1] ![0.png-261.5kB][3.2] ![file:///home/dalong/smallRoom3/seg_result/0.png][3.3] 图1为RGB图，图2为kinect扫得的深度图，图3为分割结果。 图2的深度图是使用一种HHA的算法<a href="https://github.com/s-gupta/rcnn-depth/blob/master/rcnn/saveHHA.m">[Github链接]</a>得到的.HHA的输入是kinect的深度图，输出就是以上的特征图(see HHA features from Gupta et al. ECCV14.)</p> <ul> <li>这份工作使用的数据集为nyudv2数据集<strong>（这是一个室内场景RGB-D的数据集，数据集提供了1400+的像素级的标注图片，以及大量的未标注图片）</strong>，文章使用的数据集不是全部的nyuvdv2标注类别，而是其中的40类,所以要download文章所用的数据集如下。 <blockquote> <p>While there are many labels, we follow the 40 class task defined by Perceptual Organization and Recognition of Indoor Scenes from RGB-D Images. Saurabh Gupta, Pablo Arbelaez, and Jitendra Malik. CVPR 2013 at http://www.cs.berkeley.edu/~sgupta/pdf/GuptaArbelaezMalikCVPR13.pdf . To reproduce the results of our paper, you must make use of the data from Gupta et al. at http://people.eecs.berkeley.edu/~sgupta/cvpr13/data.tgz .</p> </blockquote> </li> <li>截至2016.11.7,文章的github仍然在更新，虽然有release的版本，但文件目录结构整理不完善，需要一些debug才能跑通。 [3.1]: http://static.zybuluo.com/lrl940607/d5cy58os8dgtirsjptt4mv9i/0.png [3.2]: http://static.zybuluo.com/lrl940607/onzrx9lwj4qdz2tiddx7hzys/0.png [3.3]: http://static.zybuluo.com/lrl940607/gydmcahn66gb5b7k5tjkn77k/0.png —</li> </ul> <h3 id="instance-aware-semantic-segmentation-via-multi-task-network-cascadesmnc-cvpr-2016github链接"><strong>Instance-aware Semantic Segmentation via Multi-task Network Cascades(MNC) @CVPR 2016</strong><a href="https://github.com/Oh233">[Github链接]</a></h3> <div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>@inproceedings{dai2016instance,
    title={Instance-aware Semantic Segmentation via Multi-task Network Cascades},
    author={Dai, Jifeng and He, Kaiming and Sun, Jian},
    booktitle={CVPR},
    year={2016}
}
</code></pre></div></div> <ul> <li> <p>won the first place in COCO segmentation challenge 2015, and test at a fraction of a second per image <img src="http://static.zybuluo.com/lrl940607/7ca42ga3o8wu1kfb5ondiw40/Screenshot%20from%202016-11-22%2020:13:28.png" alt="Screenshot from 2016-11-22 20:13:28.png-392.5kB" /> <img src="http://static.zybuluo.com/lrl940607/jhxjiacbec36zv59f785uisa/Screenshot%20from%202016-11-22%2020:14:15.png" alt="Screenshot from 2016-11-22 20:14:15.png-15.4kB" /></p> </li> </ul> <hr /> <h3 id="learning-rich-features-from-rgb-d-images-for-object-detection-and-segmentationrcnn-depth"><strong>Learning Rich Features from RGB-D Images for Object Detection and Segmentation(rcnn-depth)</strong></h3> <p><a href="https://github.com/s-gupta/rcnn-depth">[Github]</a></p> <div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>@incollection{guptaECCV14,
  author = {Saurabh Gupta and Ross Girshick and Pablo Arbelaez and Jitendra Malik},
  title = {Learning Rich Features from {RGB-D} Images for Object Detection and Segmentation},
  booktitle = ECCV,
  year = {2014},
}
</code></pre></div></div> <ul> <li>HHA的出处</li> </ul> <h3 id="detectionregionobject-proposal-方法综述文章-2015"><strong>detection/region/object proposal 方法综述文章 【2015】</strong></h3> <p>http://blog.csdn.net/zxdxyz/article/details/46119369 以上都是可以找到源码的方法。 数据集方面，作者在PASCAL VOC07和ImagNet Detection dataset上面做了测试。</p> <hr /> <h3 id="py-faster-rcnn"><strong>py-faster-rcnn</strong></h3> <hr /> <h3 id="ssd"><strong>ssd</strong></h3> <hr /> <h3 id="semanticfusion-dense-3d-semantic-mapping-with-convolutional-neural-networkspdf"><strong>SemanticFusion: Dense 3D Semantic Mapping with Convolutional Neural Networks</strong><a href="https://arxiv.org/pdf/1609.05130v2.pdf">[PDF]</a></h3> <p>![屏幕快照 2016-11-11 下午9.42.28.png-717kB][4.1]</p> <div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>B. CNN Architecture
    基于caffe，采用Noh et. al.的 Deconvolutional  Semantic  Segmentation 网络结构，Deconv-S-S网络基于VGG16，但是增加了额外的max unpooling和deconvolutional layers。Deconv-S-S网络是基于RGB的输入，本文在此基础上调整代码使其接受4通道的输入
    nyudv2只有795张标注了训练图片，所以the  depth  modality  lacks the large scale training datasets of its RGB counterpart.文章initialized the depth filters with the  average  intensity  of  the  other  three  inputs。and  converted  it from  the  0–255  colour  range  to  the  0–8m  depth  range  by increasing the weights by a factor of 32
    图片resize到224*224后进入CNN（using  bilinear  interpolation  for  RGB, and  nearest  neighbour  for  depth），用Eigen et. al.实现的代码时，是resize到320*240.输出时上采样到640*480（nearest neighbour）给surfels
    
A. Network Training 
    用Noh et.al.（在PASCAL VOC2012上训练）的weights来初始化CNNs网络
    在NYUDv2训练集上用Couprie et.al.[1]定义的13类finetuned网络。用标准的随机梯度下降来优化，learning rate=0.01，momentum=0.9，weight decay=0.0005，在10k次迭代之后learning rate下降到0.001
    训练参数：mini-batch=64，迭代次数20K，GPU=Nvidia GTX Titan X.耗时2天。【注：为啥这么久？】
    
B. Reconstruction Dataset
    文章自制了一个 small experimental RGB-D reconstruction dataset，用于重建一个办公室场景，数据相比NYUDv2更普遍，更具代表性，而不是只有一个简单的背景。
    文章开发了一个标注工具来标注13类数据。
    每100帧提取一帧作为验证测试帧。一共49个验证测试帧。
    文章给出两个test result table，一个是基于他们自制的reconstruction Dataset，一个是基于NYUDv2.
    
C. CNN and CRF Update Frequency Experiments
    以RGB-CNN为测试网络
    在dataset上每2^n帧做预测，检验系统的run-time和准确率的关系。在每帧都做的时候（n=0），平均准确率由52.5%，最高，帧率8.2Hz，在128帧做一次检测时，平均准确率在45%+左右，帧率可达到30Hz+。
    Frames skipped by CRF = 500 时候对平均准确率有些许的改善，最高。太大会下降。【注：CRF是啥？】

D. Accuracy Evaluation
    加入3D Slam之后，pixel-wise的prediction准确率比只做RGB单帧prediction的准确率要高4-5个点
</code></pre></div></div> <hr /> <h3 id="learning-deconvolution-network-for-semantic-segmentationproject-pagegithub"><strong>Learning Deconvolution Network for Semantic Segmentation</strong><a href="http://cvlab.postech.ac.kr/research/deconvnet/">[Project page]</a><a href="https://github.com/HyeonwooNoh/DeconvNet">[Github]</a></h3> <div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>@inproceedings{noh2015learning,
  title={Learning Deconvolution Network for Semantic Segmentation},
  author={Noh, Hyeonwoo and Hong, Seunghoon and Han, Bohyung},
  booktitle={Computer Vision (ICCV), 2015 IEEE International Conference on},
  year={2015}
} 
</code></pre></div></div> <p>![network structure][5.1] ![evaluation][5.2]</p> <ul> <li>测试记录</li> </ul> <ol> <li>自带改写版caffe，里面写入了Deconvolution的相关结构</li> <li>基于VGG16网络，结构较大，需TITAN显卡的显存量</li> <li>主要代码目录结构 <ul> <li>DeconvNet <ul> <li>inference <ul> <li>cache_FCN8s_results.m</li> <li>generate_EDeconvNet_CRF_results.m</li> <li>run_demo.m</li> </ul> </li> </ul> </li> </ul> </li> <li>主要代码介绍 <em>run_demo.m</em>: ```matlab clear all; close all; clc;</li> </ol> <p>%% startup startup; config.imageset = ‘test’; config.cmap= ‘./voc_gt_cmap.mat’; config.gpuNum = 0; config.Path.CNN.caffe_root = ‘./caffe’; config.save_root = ‘./results’;</p> <p>%% cache FCN-8s results config.write_file = 1; config.Path.CNN.script_path = ‘./FCN’; config.Path.CNN.model_data = [config.Path.CNN.script_path ‘/fcn-8s-pascal.caffemodel’]; config.Path.CNN.model_proto = [config.Path.CNN.script_path ‘/fcn-8s-pascal-deploy.prototxt’]; config.im_sz = 500;</p> <p>cache_FCN8s_results(config);</p> <p>%% generate EDeconvNet+CRF results</p> <p>config.write_file = 1; config.edgebox_cache_dir = ‘./data/edgebox_cached/VOC2012_TEST’; config.Path.CNN.script_path = ‘./DeconvNet’; config.Path.CNN.model_data = [config.Path.CNN.script_path ‘/DeconvNet_trainval_inference.caffemodel’]; config.Path.CNN.model_proto = [config.Path.CNN.script_path ‘/DeconvNet_inference_deploy.prototxt’]; config.max_proposal_num = 50; config.im_sz = 224; config.fcn_score_dir = ‘./results/FCN8s’;</p> <p>generate_EDeconvNet_CRF_results(config);</p> <div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>首先用FCN8s来做预测。输入是RGB图像，最长边要resize到500才能输入。输出有两个，一个是预测图results/FCN8s/results/\*.png，为展示效果。一个是results/FCN8s/score/\*.mat，为预测voc 21类的pixel-wise的score。预测图就是逐pixel取max score的类别生成的。这一步大概0.7s/帧

然后进行EDeconvNet+CRF的预测。在这之前有一步预处理，选出一些被选可能的bbox。类似selective search预先选一些proposal bbox 出来的方法。文章使用的是ECCV 2014的《Edge Boxes: Locating Object Proposals from Edges》方法[[Github]](https://github.com/pdollar/edges)[[一篇中文blog]](http://blog.csdn.net/wsj998689aa/article/details/39476551)。这个edge-boxes对一张图可以给出千量级的预选bbox，以及可信度及排序。文章取前50个bbox以控制预测时间。这一步很慢，大概5-20s/帧

以下是edge-boxes简介图，文章从edge的稀疏程度这个角度来生成box，具体请看以上的blog链接：
![Screenshot from 2016-11-20 15:29:59.png-889.8kB][5.3]

- 结果展示：

see : ~/DeconvNet/lab_result/results/results_fcn8s.mov

  [5.1]: http://static.zybuluo.com/lrl940607/1cyze4ay59ztwhkvohnpld0g/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202016-11-11%20%E4%B8%8B%E5%8D%8810.06.37.png
  [5.2]: http://static.zybuluo.com/lrl940607/y31uhvuowec54wneix42q06a/Screenshot%20from%202016-11-20%2015:04:50.png
  [5.3]: http://static.zybuluo.com/lrl940607/s4rf4bo1py6u8432c50xt88d/Screenshot%20from%202016-11-20%2015:29:59.png

---

### **Predicting Depth, Surface Normals and Semantic Labels with a Common Multi-Scale Convolutional Architecture**[[Project page]](http://www.cs.nyu.edu/~deigen/dnl/)
</code></pre></div></div> <p>David Eigen, Rob Fergus {deigen,fergus}@cs.nyu.edu Paper PDF (ICCV 2015)</p> <div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>![屏幕快照 2016-11-11 下午10.11.07.png-601.1kB][4.3]

- 配置有点麻烦，装了好多东西，bug不断，不过error网上都可以解决。
- Requirements
    * theano
    * numpy, scipy
    * PIL or Pillow
- 先预测Depth和Normals，然后【image，Normals，Depth】一起预测Labels。每帧的处理时间大概20s/step。
- 文章主要是focus在normals，所以label的结果看上去并不怎么样。
- 两个step的网络输入都是320*240，输出是147*109,所以就会产生一个问题，就是第一步得到的Depth和Normals要自行resize后才可以扔到第二个网络里。demo code中没有给这一步，而是自带了一个文件包作为第二个网络的输入。
- 还有很奇怪的一点是，在预测label的demo code中发现自带的输入文件中Normals这个图片是有一圈【127，127，127】的灰度像素的，像素的宽度上下左右都不对称，这点不能理解/。
- 以及，用lab的数据测试后这个文章的自带model对于分割的效果并不好。

上结果图：
自带的demo图，label分40类：
![demo][6.2]
lab中的0.png测试图。label分40类：
![demolab40][6.3]
lab中的0.png测试图。label分13类：
![demolab13][6.4]

其中，文章给了3个model，分别对应以下三篇paper中定义的室内场景的4类，13类，以及40类：
4类：[[code&amp;&amp;data]](http://cs.nyu.edu/~silberman/projects/indoor_scene_seg_sup.html)

    "Ground in pink, Furniture in Purple, Props in Blue, Structure in Yellow"
    Indoor Segmentation and Support Inference from RGBD Images
    ECCV 2012
    
13类：

    Indoor Semantic Segmentation using depth information
    CVPR 2013
    
![Screenshot from 2016-11-23 20:10:03.png-14.5kB][6.5]

40类：
    
    NYUD-v2

  [6.1]: http://static.zybuluo.com/lrl940607/pqyzkh7rz02ewolckn76to7k/Screenshot%20from%202016-11-23%2014:46:45.png
  [6.2]: http://static.zybuluo.com/lrl940607/d10pl3nqvr8rpc8bsllapfa0/Screenshot%20from%202016-11-23%2019:55:14.png
  [6.3]: http://static.zybuluo.com/lrl940607/hiiinzivek5etmui2gmm7cn0/Screenshot%20from%202016-11-23%2019:59:38.png
  [6.4]: http://static.zybuluo.com/lrl940607/1txa5ihrcnq4cgnmk3g0rimv/Screenshot%20from%202016-11-23%2020:00:28.png
  [6.5]: http://static.zybuluo.com/lrl940607/h8yncq5za36vlf10nlppvi3z/Screenshot%20from%202016-11-23%2020:10:03.png

---

### **Indoor Semantic Segmentation using depth information**[[Author page]](http://perso.esiee.fr/~coupriec/publications.html)[[PDF]](https://arxiv.org/pdf/1301.3572v2.pdf)
</code></pre></div></div> <p>C. Couprie, C. Farabet, L. Najman, and Y. LeCun, “Indoor semantic segmentation using depth information,” in Proceedings of the Interna- tional Conference on Learning Representations (ICLR), 2013</p> <div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>![屏幕快照 2016-11-11 下午10.26.53.png-627.5kB][4.4]
---

### **Dense 3D Semantic Mapping of Indoor Scenes from RGB-D Images**[[Project Page]](http://www.vision.rwth-aachen.de/publication/0020/)
![Screenshot from 2016-11-14 12:20:40.png-126.3kB][4.5]
</code></pre></div></div> <p>2014 ICRA Best Vision Paper Alexander Hermans, Georgios Floros, Bastian Leibe “Dense 3D Semantic Mapping of Indoor Scenes from RGB-D Images” ``` [4.1]: http://static.zybuluo.com/lrl940607/7t0trfuew4vpuni29gh4dmim/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202016-11-11%20%E4%B8%8B%E5%8D%889.42.28.png [4.3]: http://static.zybuluo.com/lrl940607/xsnzdzic7rbt0srfvjeo9p6s/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202016-11-11%20%E4%B8%8B%E5%8D%8810.11.07.png [4.4]: http://static.zybuluo.com/lrl940607/lsp6nidtshaxmlyrmw5f2kbu/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202016-11-11%20%E4%B8%8B%E5%8D%8810.26.53.png [4.5]: http://static.zybuluo.com/lrl940607/wucphfx2o7dlq5u4e6lrvrp4/Screenshot%20from%202016-11-14%2012:20:40.png</p> </article> <div class="post-share"> <div class="container"> <a href="https://twitter.com/share?url=http://localhost:4000/2017/01/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%95%B4%E7%90%86.html&text=深度学习相关整理" target="_blank" class="post-share-icon twitter"></a> <a href="https://www.evernote.com/clip.action?url=http://localhost:4000/2017/01/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%95%B4%E7%90%86.html&title=深度学习相关整理" target="_blank" class="post-share-icon evernote"></a> <a href="http://service.weibo.com/share/share.php?url=http://localhost:4000/2017/01/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%95%B4%E7%90%86.html&title=深度学习相关整理" target="_blank" class="post-share-icon weibo"></a> </div> </div> <div class="comment container"> <div id="disqus_thread"> </div> </div> <div class="footer"> <div class="container"> <p class="footer-entry">All content is licensed under <a href="http://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA</a></p> <p class="footer-entry">Buit with <a href="http://jekyllrb.com/" target="_blank">Jekyll</a> and <a href="https://github.com/P233/3-Jekyll" target="_blank">3-Jekyll theme</a> • Hosted on <a href="https://pages.github.com/" target="_blank">Github</a></p> </div> </div> </main> <button id="menu" class="open"> <span id="menu-icons"></span> </button> <button id="post-toc-menu"> <span id="post-toc-menu-icons"></span> </button> <div id="post-toc"> <span id="post-toc-title">Table of Contents</span> <ul id="post-toc-ul"></ul> </div> <script src="/assets/js/jquery-2.1.3.min.js"></script> <script src="/assets/js/jquery.pjax.js"></script> <script src="/assets/js/nprogress.js"></script> <script src="/assets/js/main.js"></script> <script> (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){ (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o), m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m) })(window,document,'script','//www.google-analytics.com/analytics.js','ga'); ga('create', 'UA-54767140-1', 'yansu.org'); ga('send', 'pageview'); </script> </body> </html>
