<!DOCTYPE html> <html> <head> <meta charset="utf-8"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" /> <!--[if lte IE 9]><meta http-equiv="refresh" content="0;url=/ie.html"><![endif]--> <meta name="author" content="Ruilong Li"> <meta name="keywords" content="cs231n"> <link href='http://fonts.googleapis.com/css?family=Open+Sans:400,400italic,600|Droid+Sans+Mono' rel='stylesheet' type='text/css'> <link rel="shortcut icon" href="" /> <link rel="stylesheet" href="/css/main.css"> <link rel="canonical" href="http://localhost:4000/2017/01/09/cs231n-%E6%96%AF%E5%9D%A6%E7%A6%8F%E8%AF%BE%E7%A8%8B.html"> <!-- Begin Jekyll SEO tag v2.5.0 --> <title>cs231n 斯坦福课程 | Dalong’s Blog</title> <meta name="generator" content="Jekyll v3.8.3" /> <meta property="og:title" content="cs231n 斯坦福课程" /> <meta name="author" content="Ruilong Li" /> <meta property="og:locale" content="en_US" /> <meta name="description" content="相关资料 cs231n部分课程翻译–更新中" /> <meta property="og:description" content="相关资料 cs231n部分课程翻译–更新中" /> <link rel="canonical" href="http://localhost:4000/2017/01/09/cs231n-%E6%96%AF%E5%9D%A6%E7%A6%8F%E8%AF%BE%E7%A8%8B.html" /> <meta property="og:url" content="http://localhost:4000/2017/01/09/cs231n-%E6%96%AF%E5%9D%A6%E7%A6%8F%E8%AF%BE%E7%A8%8B.html" /> <meta property="og:site_name" content="Dalong’s Blog" /> <meta property="og:type" content="article" /> <meta property="article:published_time" content="2017-01-09T00:00:00+08:00" /> <script type="application/ld+json"> {"description":"相关资料 cs231n部分课程翻译–更新中","author":{"@type":"Person","name":"Ruilong Li"},"@type":"BlogPosting","url":"http://localhost:4000/2017/01/09/cs231n-%E6%96%AF%E5%9D%A6%E7%A6%8F%E8%AF%BE%E7%A8%8B.html","headline":"cs231n 斯坦福课程","dateModified":"2017-01-09T00:00:00+08:00","datePublished":"2017-01-09T00:00:00+08:00","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/2017/01/09/cs231n-%E6%96%AF%E5%9D%A6%E7%A6%8F%E8%AF%BE%E7%A8%8B.html"},"@context":"http://schema.org"}</script> <!-- End Jekyll SEO tag --> <link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="Dalong's Blog" /> <script src="/assets/js/prefixfree.js"></script> </head> <body> <aside id="sidebar" class="open"> <div id="sidebar-left"> <a id="sidebar-avatar" href="/"> <img id="sidebar-avatar-img" alt="" src="http://static.zybuluo.com/lrl940607/pa3uubxkdsai8ct7htwq7giu/photo.png"/> </a> <div id="sidebar-social"> <a href="/pages/feed.xml" class="sidebar-social-icon feed"></a> <a href="mailto:li-rl16@mails.tsinghua.edu.cn" class="sidebar-social-icon email"></a> <!-- Generate icon by yourself https://icomoon.io/app/#/select --> <a href="https://github.com/liruilong940607" class="sidebar-social-icon github" target="_blank"></a> </div> <ul id="sidebar-tags"> <li class="sidebar-tag active" data-filter="recent">newest</li> <li class="sidebar-tag" data-filter="course">course</li> <li class="sidebar-tag" data-filter="tutorial">tutorial</li> <li class="sidebar-tag" data-filter="lab">lab</li> </ul> </div> <div id="sidebar-right"> <div id="search-box"> <input id="search-input" type="text" placeholder="Search" /> </div> <nav id="toc"> <a class="toc-link" data-tags="tutorial" href="/2018/01/05/python-jupyter-%E5%AE%89%E8%A3%85%E7%AE%80%E6%98%8E%E6%95%99%E7%A8%8B.html"> windows python & jupyter 安装简明教程 </a> <a class="toc-link" data-tags="tutorial" href="/2017/09/01/PyQt5%E7%9A%84%E5%AE%89%E8%A3%85-Mac%E7%89%88.html"> PyQt5的安装（Mac版） </a> <a class="toc-link" data-tags="tutorial" href="/2017/09/01/Mac%E4%B8%8Bopencv2+python2.7-&-opencv3+python3.6%E7%9A%84%E9%85%8D%E7%BD%AE.html"> Mac下opencv2+python2.7 & opencv3+python3.6的配置 </a> <a class="toc-link" data-tags="tutorial" href="/2017/08/31/Mac-%E6%96%B0%E7%B3%BB%E7%BB%9F%E7%9A%84%E9%85%8D%E7%BD%AE.html"> Mac 新系统的配置 </a> <a class="toc-link" data-tags="tutorial" href="/2017/08/25/%E4%BB%8E%E4%B9%B0%E6%9C%8D%E5%8A%A1%E5%99%A8%E5%88%B0%E8%87%AA%E6%90%ADshadowsocks%E7%A7%91%E5%AD%A6%E4%B8%8A%E7%BD%91.html"> 从买服务器到自搭shadowsocks科学上网  </a> <a class="toc-link" data-tags="lab" href="/2017/07/15/%E4%BA%BA%E5%83%8F%E6%8A%A0%E5%9B%BE%E4%BB%BB%E5%8A%A1%E6%89%8B%E5%86%8Cv2.html"> 人像抠图任务手册v2 </a> <a class="toc-link" data-tags="lab" href="/2017/06/19/Flickr%E4%BA%BA%E5%83%8F%E6%8A%A0%E5%9B%BE%E4%BB%BB%E5%8A%A1-%E7%AD%9B%E5%9B%BE%E6%89%8B%E5%86%8C.html"> Flickr人像抠图任务【筛图手册】 </a> <a class="toc-link" data-tags="course" href="/2017/06/05/%E4%B8%89%E7%BB%B4%E7%82%B9%E9%9B%86%E4%B8%AD%E6%9C%80%E5%A4%A7%E7%A9%BA%E5%BF%83%E7%90%83%E7%A8%8B%E5%BA%8F%E8%AF%B4%E6%98%8E%E6%96%87%E6%A1%A3.html"> 三维点集中最大空心球程序说明文档 </a> <a class="toc-link" data-tags="lab" href="/2017/04/25/CCF-%E8%A7%86%E8%A7%89-%E5%9B%BE%E5%BD%A2%E5%AD%A6-%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD-%E9%A1%B6%E7%BA%A7%E4%BC%9A%E8%AE%AE.html"> CCF 视觉、图形学、人工智能 顶级会议 </a> <a class="toc-link" data-tags="course" href="/2017/04/16/%E9%80%89%E9%A2%98%E6%8A%A5%E5%91%8A-%E5%BE%AE%E4%BF%A1%E8%81%8A%E5%A4%A9%E8%AE%B0%E5%BD%95%E6%95%B0%E6%8D%AE%E5%8F%AF%E8%A7%86%E5%8C%96.html"> 选题报告--微信聊天记录数据可视化 </a> <a class="toc-link" data-tags="lab" href="/2017/02/18/%E5%A6%82%E4%BD%95%E5%BF%AB%E9%80%9F%E5%9C%B0%E5%81%9A%E5%9B%BE%E7%89%87%E7%9A%84%E5%89%8D%E8%83%8C%E6%99%AF%E6%A0%87%E6%B3%A8.html"> 如何快速地做图片的前背景标注？ </a> <a class="toc-link" data-tags="course" href="/2017/01/15/%E4%B8%93%E9%A2%98%E5%9B%9B-CDN%E6%95%B0%E6%8D%AE%E5%8F%AF%E8%A7%86%E5%8C%96%E4%B8%8E%E6%95%85%E9%9A%9C%E6%A3%80%E6%B5%8B.html"> 专题四：CDN数据可视化与故障检测 </a> <a class="toc-link" data-tags="lab" href="/2017/01/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%95%B4%E7%90%86.html"> 深度学习相关整理 </a> <a class="toc-link" data-tags="lab" href="/2017/01/09/cs231n-%E6%96%AF%E5%9D%A6%E7%A6%8F%E8%AF%BE%E7%A8%8B.html"> cs231n 斯坦福课程 </a> <a class="toc-link" data-tags="course" href="/2016/12/29/%E7%A5%9E%E7%BB%8F%E4%B8%8E%E8%AE%A4%E7%9F%A5-Image-Segmentation-Using-a-Sparse-Coding-Model-of-Cortical-Area-V1.html"> 神经与认知：Image Segmentation Using a Sparse Coding Model of Cortical Area V1 </a> <a class="toc-link" data-tags="course" href="/2016/12/01/SNNs(Spiking-Neural-Networks)%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0.html"> SNNs(Spiking Neural Networks)学习笔记 </a> <a class="toc-link" data-tags="lab" href="/2016/11/27/RGB-D-dataset.html"> RGB-D dataset </a> <a class="toc-link" data-tags="lab" href="/2016/11/24/SUN-RGB-D-dataset.html"> SUN RGB-D dataset </a> <a class="toc-link" data-tags="lab" href="/2016/11/24/B3DO-Berkeley-3-D-Object-Dataset.html"> (B3DO) Berkeley 3-D Object Dataset </a> <a class="toc-link" data-tags="lab" href="/2016/11/23/Paper%E7%AC%94%E8%AE%B0-Segmentation%E7%9B%B8%E5%85%B3%E7%9A%84%E4%B8%A4%E7%AF%87%E6%96%87%E7%AB%A0.html"> Paper笔记：Segmentation相关的两篇文章 </a> <a class="toc-link" data-tags="lab" href="/2016/11/22/%E8%AE%B0%E5%BD%95-%E4%BD%BF%E7%94%A8py-faster-rcnn%E5%9C%A8%E8%87%AA%E5%B7%B1%E7%9A%84%E6%95%B0%E6%8D%AE%E9%9B%86%E4%B8%8A.html"> 记录：使用py-faster-rcnn在自己的数据集上 </a> <a class="toc-link" data-tags="lab" href="/2016/11/20/Paper%E7%AC%94%E8%AE%B0-SemanticFusion.html"> Paper笔记：SemanticFusion </a> <a class="toc-link" data-tags="lab" href="/2016/11/20/Paper%E7%AC%94%E8%AE%B0-Deconvolution-Network.html"> Paper笔记：Deconvolution Network </a> <a class="toc-link" data-tags="tutorial" href="/2016/11/18/caffe-configuration-on-ubuntu-14.04.html"> caffe ubuntu 14.04 -- 2016.11.18 </a> <a class="toc-link" data-tags="course" href="/2016/11/10/%E6%B5%81%E5%AA%92%E4%BD%93%E6%8A%80%E6%9C%AF-%E4%B8%93%E9%A2%98%E5%9B%9B.html"> 流媒体技术--专题四 </a> </nav> </div> </aside> <main id="main" class="open"> <article class="post container"> <div class="post-meta"> <span class="post-meta-span date">2017 January 09</span> </div> <h1 class="post-title">cs231n 斯坦福课程</h1> <h2 id="相关资料">相关资料</h2> <ul> <li><a href="https://zhuanlan.zhihu.com/intelligentunit">cs231n部分课程翻译–更新中</a></li> </ul> <h2 id="学习顺序">学习顺序</h2> <h3 id="1-assignment1">1. <a href="http://cs231n.github.io/assignments2016/assignment1/">assignment1</a></h3> <ul> <li>Q1: k-Nearest Neighbor classifier <ul> <li>相关代码 <ul> <li>入口： http://localhost:8888/notebooks/Documents/cs231n/assignment1/knn.ipynb</li> <li>函数： http://localhost:8888/edit/Documents/cs231n/assignment1/cs231n/classifiers/k_nearest_neighbor.py</li> </ul> </li> <li>The kNN classifier consists of two stages: <ul> <li>During training, the classifier takes the training data and simply remembers it</li> <li>During testing, kNN classifies every test image by comparing to all training images and transfering the labels of the k most similar training examples(by a vote)</li> <li>The value of k is cross-validated</li> </ul> </li> <li>cross-validation <ul> <li>将训练集分成5份，用4份训练，1份预测，轮转5次，得到5个accuracy（对于每个k），取平均accuracy最大的k来确定k的取值。</li> </ul> </li> <li>28% accuracy</li> </ul> </li> <li>Q2: Training a Support Vector Machine <ul> <li>参考资料 <ul> <li>http://blog.csdn.net/yc461515457/article/details/51921607</li> </ul> </li> <li>相关代码 <ul> <li>入口：http://localhost:8888/notebooks/Documents/cs231n/assignment1/svm.ipynb</li> <li>函数：http://localhost:8888/edit/Documents/cs231n/assignment1/cs231n/classifiers/linear_svm.py</li> <li>函数：http://localhost:8888/edit/Documents/cs231n/assignment1/cs231n/classifiers/linear_classifier.py</li> </ul> </li> <li>预处理 <ul> <li>reshape the image data into rows</li> <li>compute the image mean based on the training data</li> <li>subtract the mean image from train and test data</li> <li>append the bias dimension of ones (i.e. bias trick) so that our SVM only has to worry about optimizing a single weight matrix W.</li> </ul> </li> <li>SVM 分类器 <ul> <li>参数W初始化： W = np.random.randn(3073, 10) * 0.0001 ##每张图32*32+1，10类</li> <li>计算loss和梯度dW： <img src="http://static.zybuluo.com/lrl940607/idypazjml8h13g6ckdnf01gb/loss.png" alt="loss.png-101.3kB" /> 代码实现如下（矩阵运算版本速度更快，但是loop版本更易理解）： <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="k">def</span> <span class="nf">svm_loss_naive</span><span class="p">(</span><span class="n">W</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">reg</span><span class="p">):</span>
<span class="s">"""
Structured SVM loss function, naive implementation (with loops).
        
Inputs have dimension D, there are C classes, and we operate on minibatches
of N examples.
        
Inputs:
- W: A numpy array of shape (D, C) containing weights.
- X: A numpy array of shape (N, D) containing a minibatch of data.
- y: A numpy array of shape (N,) containing training labels; y[i] = c means
  that X[i] has label c, where 0 &lt;= c &lt; C.
- reg: (float) regularization strength
        
Returns a tuple of:
- loss as single float
- gradient with respect to weights W; an array of same shape as W
"""</span>
<span class="n">dW</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">W</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="c"># initialize the gradient as zero</span>
<span class="c"># compute the loss and the gradient</span>
<span class="n">num_classes</span> <span class="o">=</span> <span class="n">W</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
<span class="n">num_train</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">loss</span> <span class="o">=</span> <span class="mf">0.0</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">xrange</span><span class="p">(</span><span class="n">num_train</span><span class="p">):</span>
  <span class="n">scores</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">W</span><span class="p">)</span>
  <span class="n">correct_class_score</span> <span class="o">=</span> <span class="n">scores</span><span class="p">[</span><span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">]]</span>
  <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">xrange</span><span class="p">(</span><span class="n">num_classes</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">j</span> <span class="o">==</span> <span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">]:</span>
      <span class="k">continue</span>
    <span class="n">margin</span> <span class="o">=</span> <span class="n">scores</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="o">-</span> <span class="n">correct_class_score</span> <span class="o">+</span> <span class="mi">1</span> <span class="c"># note delta = 1</span>
    <span class="k">if</span> <span class="n">margin</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
      <span class="n">loss</span> <span class="o">+=</span> <span class="n">margin</span>
      <span class="c"># Compute gradients (one inner and one outer sum)</span>
      <span class="c"># Wonderfully compact and hard to read</span>
      <span class="n">dW</span><span class="p">[:,</span> <span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">]]</span> <span class="o">-=</span> <span class="n">X</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="p">:]</span><span class="o">.</span><span class="n">T</span> <span class="c"># this is really a sum over j != y_i</span>
      <span class="n">dW</span><span class="p">[:,</span> <span class="n">j</span><span class="p">]</span> <span class="o">+=</span> <span class="n">X</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="p">:]</span><span class="o">.</span><span class="n">T</span> <span class="c"># sums each contribution of the x_i's</span>
        
<span class="c"># Right now the loss is a sum over all training examples, but we want it</span>
<span class="c"># to be an average instead so we divide by num_train.</span>
<span class="n">loss</span> <span class="o">/=</span> <span class="n">num_train</span>
<span class="c"># Same with gradient</span>
<span class="n">dW</span> <span class="o">/=</span> <span class="n">num_train</span>
<span class="c"># Add regularization to the loss.</span>
<span class="n">loss</span> <span class="o">+=</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">reg</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">W</span> <span class="o">*</span> <span class="n">W</span><span class="p">)</span>
<span class="c"># Gradient regularization </span>
<span class="n">dW</span> <span class="o">+=</span> <span class="n">reg</span><span class="o">*</span><span class="n">W</span>
<span class="c">#############################################################################</span>
<span class="c"># TODO:                                                                     #</span>
<span class="c"># Compute the gradient of the loss function and store it dW.                #</span>
<span class="c"># Rather that first computing the loss and then computing the derivative,   #</span>
<span class="c"># it may be simpler to compute the derivative at the same time that the     #</span>
<span class="c"># loss is being computed. As a result you may need to modify some of the    #</span>
<span class="c"># code above to compute the gradient.                                       #</span>
<span class="c">#############################################################################</span>
<span class="k">return</span> <span class="n">loss</span><span class="p">,</span> <span class="n">dW</span>
</code></pre></div> </div> </li> </ul> </li> <li>梯度下降SGD（Stochastic Gradient Descent） <ul> <li>self.W = self.W - learning_rate * grad</li> </ul> </li> <li>用验证集来tune更好的参数： <ul> <li>Use the validation set to tune hyperparameters (regularization strength and learning rate).</li> </ul> </li> <li><strong>regularization正则化方法：防止过拟合，提高泛化能力</strong> <ul> <li>参考：http://www.mamicode.com/info-detail-517504.html</li> <li>在训练数据不够多时，或者overtraining时，常常会导致overfitting（过拟合）。其直观的表现如下图所示，随着训练过程，网络在training data上的error渐渐减小，但是在验证集上的error却反而渐渐增大——因为训练出来的网络过拟合了训练集，对训练集外的数据却不work。 <img src="http://static.zybuluo.com/lrl940607/4iyyqz502ildzhz6ig5mbxr3/overfitting.png" alt="overfitting.png-33.3kB" /></li> <li>为了防止overfitting，可以用的方法有很多，有一个概念需要先说明，在机器学习算法中，我们常常将原始数据集分为三部分：training data、validation data，testing data。这个validation data是什么？它其实就是用来避免过拟合的，在训练过程中，我们通常用它来确定一些超参数（比如根据validation data上的accuracy来确定early stopping的epoch大小、根据validation data确定learning rate等等）。那为啥不直接在testing data上做这些呢？因为如果在testing data做这些，那么随着训练的进行，我们的网络实际上就是在一点一点地overfitting我们的testing data，导致最后得到的testing accuracy没有任何参考意义。因此，training data的作用是计算梯度更新权重，validation data如上所述，testing data则给出一个accuracy以判断网络的好坏。</li> <li>避免过拟合的方法有很多：early stopping、数据集扩增（Data augmentation）、正则化（Regularization）包括L1、L2（L2 regularization也叫weight decay），dropout。</li> <li> <script type="math/tex; mode=display">C=C_{0}+\frac{\lambda}{2n}\sum_{\omega}\omega^2</script> </li> <li>C0代表原始的代价函数，后面那一项就是L2正则化项，它是这样来的：所有参数w的平方的和，除以训练集的样本大小n。λ就是正则项系数，权衡正则项与C0项的比重。另外还有一个系数1/2，1/2经常会看到，主要是为了后面求导的结果方便，后面那一项求导会产生一个2，与1/2相乘刚好凑整。</li> <li>L2正则化项是怎么避免overfitting的呢？我们推导一下看看，先求导：</li> <li> <script type="math/tex; mode=display">\frac{\partial C}{\partial\omega}=\frac{\partial C_{0}}{\partial\omega}+\frac{\lambda}{n}\omega</script> </li> <li> <script type="math/tex; mode=display">\frac{\partial C}{\partial b}=\frac{\partial C_{0}}{\partial b}</script> </li> <li>可以发现L2正则化项对b的更新没有影响，但是对于w的更新有影响:</li> <li> <script type="math/tex; mode=display">\omega\rightarrow\omega-\eta\frac{\partial C_{0}}{\partial\omega}-\frac{\eta\lambda}{n}\omega=(1-\frac{\eta\lambda}{n})\omega-\eta\frac{\partial C_{0}}{\partial\omega}</script> </li> <li>在不使用L2正则化时，求导结果中w前系数为1，现在w前面系数为 1?ηλ/n ，因为η、λ、n都是正的，所以 1?ηλ/n小于1，它的效果是减小w，这也就是权重衰减（weight decay）的由来。当然考虑到后面的导数项，w最终的值可能增大也可能减小。</li> <li>另外，需要提一下，对于基于mini-batch的随机梯度下降，w和b更新的公式跟上面给出的有点不同：</li> <li> <script type="math/tex; mode=display">\omega\rightarrow(1-\frac{\eta\lambda}{n})\omega-\frac{\eta}{m}\sum_{x}\frac{\partial C_{x}}{\partial\omega}</script> </li> <li> <script type="math/tex; mode=display">b \rightarrow b-\frac{\eta}{m}\sum_{x}\frac{\partial C_{x}}{\partial b}</script> </li> <li>对比上面w的更新公式，可以发现后面那一项变了，变成所有导数加和，乘以η再除以m，m是一个mini-batch中样本的个数。</li> <li>到目前为止，我们只是解释了L2正则化项有让w“变小”的效果，但是还没解释为什么w“变小”可以防止overfitting？人们普遍认为：更小的权值w，从某种意义上说，表示网络的复杂度更低，对数据的拟合刚刚好（这个法则也叫做奥卡姆剃刀）。而在实际应用中，也验证了这一点，L2正则化的效果往往好于未经正则化的效果。</li> </ul> </li> <li>38.9% accuracy</li> </ul> </li> <li>Q3: Implement a Softmax classifier</li> <li>Q4: Two-Layer Neural Network</li> <li>Q5: Higher Level Representations: Image Features</li> <li> <p>Q6: Cool Bonus: Do something extra!</p> </li> </ul> </article> <div class="post-share"> <div class="container"> <a href="https://twitter.com/share?url=http://localhost:4000/2017/01/09/cs231n-%E6%96%AF%E5%9D%A6%E7%A6%8F%E8%AF%BE%E7%A8%8B.html&text=cs231n 斯坦福课程" target="_blank" class="post-share-icon twitter"></a> <a href="https://www.evernote.com/clip.action?url=http://localhost:4000/2017/01/09/cs231n-%E6%96%AF%E5%9D%A6%E7%A6%8F%E8%AF%BE%E7%A8%8B.html&title=cs231n 斯坦福课程" target="_blank" class="post-share-icon evernote"></a> <a href="http://service.weibo.com/share/share.php?url=http://localhost:4000/2017/01/09/cs231n-%E6%96%AF%E5%9D%A6%E7%A6%8F%E8%AF%BE%E7%A8%8B.html&title=cs231n 斯坦福课程" target="_blank" class="post-share-icon weibo"></a> </div> </div> <div class="comment container"> <div id="disqus_thread"> </div> </div> <div class="footer"> <div class="container"> <p class="footer-entry">All content is licensed under <a href="http://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA</a></p> <p class="footer-entry">Buit with <a href="http://jekyllrb.com/" target="_blank">Jekyll</a> and <a href="https://github.com/P233/3-Jekyll" target="_blank">3-Jekyll theme</a> • Hosted on <a href="https://pages.github.com/" target="_blank">Github</a></p> </div> </div> </main> <button id="menu" class="open"> <span id="menu-icons"></span> </button> <button id="post-toc-menu"> <span id="post-toc-menu-icons"></span> </button> <div id="post-toc"> <span id="post-toc-title">Table of Contents</span> <ul id="post-toc-ul"></ul> </div> <script src="/assets/js/jquery-2.1.3.min.js"></script> <script src="/assets/js/jquery.pjax.js"></script> <script src="/assets/js/nprogress.js"></script> <script src="/assets/js/main.js"></script> <script> (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){ (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o), m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m) })(window,document,'script','//www.google-analytics.com/analytics.js','ga'); ga('create', 'UA-54767140-1', 'yansu.org'); ga('send', 'pageview'); </script> </body> </html>
