---
layout: post
title: cs231n 斯坦福课程
category: lab
keywords: cs231n
---

## 相关资料
- [cs231n部分课程翻译--更新中](https://zhuanlan.zhihu.com/intelligentunit)

## 学习顺序
### 1. [assignment1](http://cs231n.github.io/assignments2016/assignment1/)
- Q1: k-Nearest Neighbor classifier 
    - 相关代码
        - 入口： http://localhost:8888/notebooks/Documents/cs231n/assignment1/knn.ipynb
        - 函数： http://localhost:8888/edit/Documents/cs231n/assignment1/cs231n/classifiers/k_nearest_neighbor.py
    - The kNN classifier consists of two stages:
        - During training, the classifier takes the training data and simply remembers it
        - During testing, kNN classifies every test image by comparing to all training images and transfering the labels of the k most similar training examples(by a vote)
        - The value of k is cross-validated
    - cross-validation
        - 将训练集分成5份，用4份训练，1份预测，轮转5次，得到5个accuracy（对于每个k），取平均accuracy最大的k来确定k的取值。  
    - 28% accuracy
- Q2: Training a Support Vector Machine
    - 参考资料
        - http://blog.csdn.net/yc461515457/article/details/51921607
    - 相关代码
        - 入口：http://localhost:8888/notebooks/Documents/cs231n/assignment1/svm.ipynb
        - 函数：http://localhost:8888/edit/Documents/cs231n/assignment1/cs231n/classifiers/linear_svm.py
        - 函数：http://localhost:8888/edit/Documents/cs231n/assignment1/cs231n/classifiers/linear_classifier.py
    - 预处理
        - reshape the image data into rows
        - compute the image mean based on the training data
        - subtract the mean image from train and test data
        - append the bias dimension of ones (i.e. bias trick) so that our SVM only has to worry about optimizing a single weight matrix W.
    - SVM 分类器
        - 参数W初始化： W = np.random.randn(3073, 10) * 0.0001 ##每张图32*32+1，10类
        - 计算loss和梯度dW：
        ![loss.png-101.3kB][1]
        代码实现如下（矩阵运算版本速度更快，但是loop版本更易理解）：
        ```python
        def svm_loss_naive(W, X, y, reg):
          """
          Structured SVM loss function, naive implementation (with loops).
        
          Inputs have dimension D, there are C classes, and we operate on minibatches
          of N examples.
        
          Inputs:
          - W: A numpy array of shape (D, C) containing weights.
          - X: A numpy array of shape (N, D) containing a minibatch of data.
          - y: A numpy array of shape (N,) containing training labels; y[i] = c means
            that X[i] has label c, where 0 <= c < C.
          - reg: (float) regularization strength
        
          Returns a tuple of:
          - loss as single float
          - gradient with respect to weights W; an array of same shape as W
          """
          dW = np.zeros(W.shape) # initialize the gradient as zero
          # compute the loss and the gradient
          num_classes = W.shape[1]
          num_train = X.shape[0]
          loss = 0.0
          for i in xrange(num_train):
            scores = X[i].dot(W)
            correct_class_score = scores[y[i]]
            for j in xrange(num_classes):
              if j == y[i]:
                continue
              margin = scores[j] - correct_class_score + 1 # note delta = 1
              if margin > 0:
                loss += margin
                # Compute gradients (one inner and one outer sum)
                # Wonderfully compact and hard to read
                dW[:, y[i]] -= X[i, :].T # this is really a sum over j != y_i
                dW[:, j] += X[i, :].T # sums each contribution of the x_i's
        
          # Right now the loss is a sum over all training examples, but we want it
          # to be an average instead so we divide by num_train.
          loss /= num_train
          # Same with gradient
          dW /= num_train
          # Add regularization to the loss.
          loss += 0.5 * reg * np.sum(W * W)
          # Gradient regularization 
          dW += reg*W
          #############################################################################
          # TODO:                                                                     #
          # Compute the gradient of the loss function and store it dW.                #
          # Rather that first computing the loss and then computing the derivative,   #
          # it may be simpler to compute the derivative at the same time that the     #
          # loss is being computed. As a result you may need to modify some of the    #
          # code above to compute the gradient.                                       #
          #############################################################################
          return loss, dW
        ```
    - 梯度下降SGD（Stochastic Gradient Descent）
        -  self.W = self.W - learning_rate * grad
    - 用验证集来tune更好的参数：
        - Use the validation set to tune hyperparameters (regularization strength and learning rate). 
    - **regularization正则化方法：防止过拟合，提高泛化能力**
        - 参考：http://www.mamicode.com/info-detail-517504.html
        - 在训练数据不够多时，或者overtraining时，常常会导致overfitting（过拟合）。其直观的表现如下图所示，随着训练过程，网络在training data上的error渐渐减小，但是在验证集上的error却反而渐渐增大——因为训练出来的网络过拟合了训练集，对训练集外的数据却不work。
        ![overfitting.png-33.3kB][2]
        - 为了防止overfitting，可以用的方法有很多，有一个概念需要先说明，在机器学习算法中，我们常常将原始数据集分为三部分：training data、validation data，testing data。这个validation data是什么？它其实就是用来避免过拟合的，在训练过程中，我们通常用它来确定一些超参数（比如根据validation data上的accuracy来确定early stopping的epoch大小、根据validation data确定learning rate等等）。那为啥不直接在testing data上做这些呢？因为如果在testing data做这些，那么随着训练的进行，我们的网络实际上就是在一点一点地overfitting我们的testing data，导致最后得到的testing accuracy没有任何参考意义。因此，training data的作用是计算梯度更新权重，validation data如上所述，testing data则给出一个accuracy以判断网络的好坏。
        - 避免过拟合的方法有很多：early stopping、数据集扩增（Data augmentation）、正则化（Regularization）包括L1、L2（L2 regularization也叫weight decay），dropout。
        - $$ C=C_{0}+\frac{\lambda}{2n}\sum_{\omega}\omega^2 $$
        - C0代表原始的代价函数，后面那一项就是L2正则化项，它是这样来的：所有参数w的平方的和，除以训练集的样本大小n。λ就是正则项系数，权衡正则项与C0项的比重。另外还有一个系数1/2，1/2经常会看到，主要是为了后面求导的结果方便，后面那一项求导会产生一个2，与1/2相乘刚好凑整。
        - L2正则化项是怎么避免overfitting的呢？我们推导一下看看，先求导：
        - $$ \frac{\partial C}{\partial\omega}=\frac{\partial C_{0}}{\partial\omega}+\frac{\lambda}{n}\omega $$
        - $$ \frac{\partial C}{\partial b}=\frac{\partial C_{0}}{\partial b} $$
        - 可以发现L2正则化项对b的更新没有影响，但是对于w的更新有影响:
        - $$ \omega\rightarrow\omega-\eta\frac{\partial C_{0}}{\partial\omega}-\frac{\eta\lambda}{n}\omega=(1-\frac{\eta\lambda}{n})\omega-\eta\frac{\partial C_{0}}{\partial\omega} $$
        - 在不使用L2正则化时，求导结果中w前系数为1，现在w前面系数为 1?ηλ/n ，因为η、λ、n都是正的，所以 1?ηλ/n小于1，它的效果是减小w，这也就是权重衰减（weight decay）的由来。当然考虑到后面的导数项，w最终的值可能增大也可能减小。
        - 另外，需要提一下，对于基于mini-batch的随机梯度下降，w和b更新的公式跟上面给出的有点不同：
        - $$ \omega\rightarrow(1-\frac{\eta\lambda}{n})\omega-\frac{\eta}{m}\sum_{x}\frac{\partial C_{x}}{\partial\omega} $$
        - $$ b \rightarrow b-\frac{\eta}{m}\sum_{x}\frac{\partial C_{x}}{\partial b} $$
        - 对比上面w的更新公式，可以发现后面那一项变了，变成所有导数加和，乘以η再除以m，m是一个mini-batch中样本的个数。
        - 到目前为止，我们只是解释了L2正则化项有让w“变小”的效果，但是还没解释为什么w“变小”可以防止overfitting？人们普遍认为：更小的权值w，从某种意义上说，表示网络的复杂度更低，对数据的拟合刚刚好（这个法则也叫做奥卡姆剃刀）。而在实际应用中，也验证了这一点，L2正则化的效果往往好于未经正则化的效果。
    - 38.9% accuracy
- Q3: Implement a Softmax classifier
- Q4: Two-Layer Neural Network
- Q5: Higher Level Representations: Image Features
- Q6: Cool Bonus: Do something extra!


  [1]: http://static.zybuluo.com/lrl940607/idypazjml8h13g6ckdnf01gb/loss.png
  [2]: http://static.zybuluo.com/lrl940607/4iyyqz502ildzhz6ig5mbxr3/overfitting.png